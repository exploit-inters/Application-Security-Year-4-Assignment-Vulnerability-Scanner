import sys
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse, parse_qs
import urllib3
from requests.adapters import HTTPAdapter
from requests.packages.urllib3.util.retry import Retry

#Describes a spider with a set of parameters deciding what to spider and a set of
#urls found by running the spider, and functions to run the spider.
class Spider:
    #The url to start the spider on
    start_url = ''
    #the string depicting the aproximate domain the spider should stay in
    domain = ''
    #A list of strings to avoid in potential links
    deny_strs = []
    #The path to the file to output the found urls to
    urls_file_name = ''
    #The list of urls already checked
    done = []
    #the list of urls left to check for links
    to_do = []
    #The list of urls found
    urls = []
    #Whether or not a parameter was specified that must be unique in found urls
    is_params_restricted = False
    #The name of the parameter that must be unique
    param_restricted = ''
    #The values found for the unique requirement parameter
    params_restricted = []

    #Create a new spider with the supplied details or defaults
    def __init__(self, start_url, domain, deny_strs = None, param_restricted = None, urls_file_name = 'urls_spidered.txt'):
        #Set up all of the variables supplied as the spider's variables
        self.start_url = start_url
        self.domain = domain
        self.urls_file_name = urls_file_name
        self.to_do.append(start_url)
        if deny_strs is not None:
            self.deny_strs = deny_strs
        if param_restricted is not None:
            self.is_params_restricted = True
            self.param_restricted = param_restricted
        #Create a session for the requests that will be made, with 2 total retries
        self.s = requests.Session()
        self.s.mount(start_url, HTTPAdapter(max_retries=(Retry(total=2, backoff_factor=0.1))))
        #Don't require that the program verify the credentials of https connections, and hide
        #warnings about unverified credentials
        self.s.verify=False
        urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
        #print a message to the user that the spider was created and show the details
        print('\nCreated spider, Details:\n' + '-'*25 + '\n' + self.to_string() + '\n' + '-'*25 + '\n')

    #Make a string out of the details of the spider
    def to_string(self):
        string = ('Start url = ' + self.start_url + '\nDomain = ' + self.domain
        + '\nDenied Strings = ' + str(self.deny_strs) + '\nRestricted Paremeter: '
        + self.param_restricted + '\nURL output file: ' + self.urls_file_name)
        return string

    #Start the spider running
    def start_spider(self):
        #Tell the user it was started
        print('Starting spider at ' + self.start_url + '\n')
        #Get every link from the robots.txt file
        self.get_robots()
        #While there are links to check, check them
        while len(self.to_do) > 0:
            self.do_spider()
        #Tell the user the final total of urls checked and clear the line
        sys.stdout.write("\r([ done: {:5d} | todo: {:5d} ]) {:100s}\n".format(len(self.done), len(self.to_do), ' '*100))
        sys.stdout.flush()
        #write the urls found to a text file
        urls_file = open(self.urls_file_name, 'w+')
        urlsstr = '\n'.join(self.urls)
        urls_file.write(urlsstr)
        #Tell the user the spider is done and where the results are
        print ('Done, ' + str(len(self.done)) + ' pages scanned. ' + str(len(self.urls)) + ' urls written to ' + self.urls_file_name)

    #Decide whether or not to check the page for links, move the url to the
    #done list
    def do_spider(self):
        #Get the url at the top of the to_do list
        page_url = self.to_do[0]
        #Tell the user how far the spider has gotten and what it has left to do so
        #they know it is running and not frozen
        self.show_progress(page_url)
        #Make sure the page hasn't already been checked
        if page_url not in self.done:
            #make sure the url doesn't contain a duplicate of the restricted parameter
            if not self.is_params_restricted or (self.is_params_restricted and not self.check_param(page_url)):
                    #check for links
                    self.process_page(page_url)
            #Add the url to the list of checked pages
            self.done.append(page_url)
        #remove the url from the to_do list
        self.to_do.remove(page_url)

    #Check for links in the page and add valid ones to the to_do list
    #Check if the page can be reached before adding it to the urls found
    def process_page(self, page_url):
        try:
            #get the page
            page = self.s.get(page_url)
            #If it can be reached, add it to the list of urls found
            if page.status_code == 200:
                self.urls.append(page_url)
                #Find all the links in the page
                soup = BeautifulSoup(page.text, 'html.parser')
                links = soup.find_all('a', href=True)
                #check if each link is valid and add to the to_do list
                for link in links:
                    #Start by denying the link, set to false if valid later
                    deny = True
                    h = link['href']
                    href = urljoin(self.start_url, h)
                    #If it's not already in the done list or to_do list and it is in the domain
                    if (href not in self.to_do) and (href not in self.done) and (self.domain in href):
                        #don't deny the link, so far it's valid
                        deny = False
                        #If any of the denied strings are in the link, deny it
                        for deny_str in self.deny_strs:
                            if deny_str in href:
                                deny = True
                    #If it was valid and not containing denied strings, add to the to_do list
                    if deny == False:
                        self.to_do.append(href)
        except Exception as e:
            print('error in ' + page_url + '\n' + repr(e))

    #tell the user how the spider is progressing - the number of links checked
    #and the number of links currently in the to_do list
    def show_progress(self, page_url):
        urlstr = page_url[-80:] if len(page_url) > 80 else page_url.ljust(80)
        sys.stdout.write("([ done: {:5d} | todo: {:5d} ]) Scanning {:80s}\r".format(len(self.done), len(self.to_do), urlstr))
        sys.stdout.flush()

    #Check if there is a restricted parameter in the url
    #If there is, check if the value has been added already
    #if it has, tell the program to ignore that link
    #if not, add the value and tell the program to continue checking that url
    def check_param(self, page_url):
        paramstr = urlparse(page_url).query
        params = parse_qs(paramstr)
        if self.param_restricted in params:
            if params[self.param_restricted] not in self.params_restricted:
                self.params_restricted.append(params[self.param_restricted])
                return False
            return True
        return False

    #Check the robots.txt page for allowed or disallowed links to add to the to_do list
    def get_robots(self):
        #get the address of a potential robots.txt
        robots = self.start_url + ('' if self.start_url.endswith('/') else '/') + 'robots.txt'
        text = requests.get(robots).text
        #make a list of all of the lines in robots.txt
        listing = text.splitlines()
        results = []
        #if the line starts with allow or disallow, add the url to the results
        for line in listing:
            if line.startswith('Allow') or line.startswith('Disallow'):
                results.append(self.start_url + ('' if self.start_url.endswith('/') else '/') + line.split(': ')[1].split(' ')[0])
        #check every link from robots.txt for validity before adding to the to_do list
        for page_url in results:
            deny = True
            if page_url not in self.done:
                if not self.is_params_restricted or (self.is_params_restricted and not self.check_param(page_url)):
                    if (page_url not in self.to_do) and (page_url not in self.done):
                        deny = False
                        for deny_str in self.deny_strs:
                            if deny_str in page_url:
                                deny = True
                    if deny == False:
                        self.to_do.append(page_url)
